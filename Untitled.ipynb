{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubham-sharma2005/CODSOFT/blob/main/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OzyrkQA-sRUZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFgETxkgsU9k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e60d7eba"
      },
      "source": [
        "# Task\n",
        "Build a model to detect fraudulent credit card transactions using the dataset from \"/content/fraudTest.csv\". Experiment with Logistic Regression, Decision Trees, and Random Forests, and evaluate their performance to select the best model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57e43ffc"
      },
      "source": [
        "## Load data\n",
        "\n",
        "### Subtask:\n",
        "Load the dataset from \"/content/fraudTest.csv\" into a pandas DataFrame and display the first 5 rows and the columns and their types to discern how to proceed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a770475f"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the dataset into a pandas DataFrame and display the first few rows and the column information.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2c83dc3"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/fraudTest.csv')\n",
        "display(df.head())\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4f178b0"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Handle missing values and encode categorical features if any exist.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eebcd0b"
      },
      "source": [
        "**Reasoning**:\n",
        "Identify columns with missing values and handle them by dropping rows with missing values in `is_fraud` and imputing numerical columns with the mean. Then identify categorical columns and apply one-hot encoding to relevant ones, dropping identifier columns. Finally, display the info of the processed dataframe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9908363c"
      },
      "source": [
        "# Identify columns with missing values\n",
        "print(\"Columns with missing values before handling:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Handle missing values in 'is_fraud' by dropping the row\n",
        "df.dropna(subset=['is_fraud'], inplace=True)\n",
        "\n",
        "# Handle missing values in numerical columns by imputing with the mean\n",
        "numerical_cols_to_impute = ['unix_time', 'merch_lat', 'merch_long']\n",
        "for col in numerical_cols_to_impute:\n",
        "    if col in df.columns:\n",
        "        mean_value = df[col].mean()\n",
        "        df[col].fillna(mean_value, inplace=True)\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "print(\"\\nCategorical columns before encoding:\")\n",
        "print(categorical_cols)\n",
        "\n",
        "# Drop identifier columns that are not useful for modeling\n",
        "cols_to_drop = ['Unnamed: 0', 'cc_num', 'trans_num', 'merchant']\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# Apply one-hot encoding to relevant categorical columns\n",
        "# Exclude columns that were just dropped\n",
        "categorical_cols_to_encode = [col for col in categorical_cols if col not in cols_to_drop]\n",
        "\n",
        "if categorical_cols_to_encode:\n",
        "    df = pd.get_dummies(df, columns=categorical_cols_to_encode, drop_first=True)\n",
        "\n",
        "# Verify that there are no remaining missing values and check data types\n",
        "print(\"\\nDataFrame info after handling missing values and encoding:\")\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cab4aa01"
      },
      "source": [
        "## Feature engineering\n",
        "\n",
        "### Subtask:\n",
        "Create new features that might help improve the model's performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4be8d77"
      },
      "source": [
        "**Reasoning**:\n",
        "The instructions require creating new features. I will start by converting `unix_time` to datetime, then extract time-based features. After that, I will calculate the distance between merchant and transaction locations using the Haversine formula. Finally, I will calculate the time difference between consecutive transactions for each card.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38baf123"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Convert unix_time to datetime\n",
        "df['datetime'] = pd.to_datetime(df['unix_time'], unit='s')\n",
        "\n",
        "# 2. Extract time-based features\n",
        "df['hour'] = df['datetime'].dt.hour\n",
        "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "df['month'] = df['datetime'].dt.month\n",
        "\n",
        "# 3. Calculate distance using Haversine formula\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "\n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "\n",
        "    distance = R * c\n",
        "    return distance\n",
        "\n",
        "df['distance_to_merchant'] = df.apply(lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1)\n",
        "\n",
        "# 4. Create feature for time difference between consecutive transactions\n",
        "df['time_diff_consecutive'] = df.groupby('card_present_flag')['datetime'].diff().dt.total_seconds().fillna(0)\n",
        "\n",
        "# 5. Display the first 5 rows and check data types\n",
        "display(df.head())\n",
        "print(df[['datetime', 'hour', 'day_of_week', 'month', 'distance_to_merchant', 'time_diff_consecutive']].info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efc73cee"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the pandas library was not imported. I need to re-run the code block with the pandas library imported at the beginning. I will also keep the numpy import as it is used for calculating the distance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wrZb91d7sQQ"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Convert unix_time to datetime\n",
        "df['datetime'] = pd.to_datetime(df['unix_time'], unit='s')\n",
        "\n",
        "# 2. Extract time-based features\n",
        "df['hour'] = df['datetime'].dt.hour\n",
        "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "df['month'] = df['datetime'].dt.month\n",
        "\n",
        "# 3. Calculate distance using Haversine formula\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "\n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "\n",
        "    distance = R * c\n",
        "    return distance\n",
        "\n",
        "df['distance_to_merchant'] = df.apply(lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1)\n",
        "\n",
        "# 4. Create feature for time difference between consecutive transactions\n",
        "df['time_diff_consecutive'] = df.groupby('card_present_flag')['datetime'].diff().dt.total_seconds().fillna(0)\n",
        "\n",
        "# 5. Display the first 5 rows and check data types\n",
        "display(df.head())\n",
        "print(df[['datetime', 'hour', 'day_of_week', 'month', 'distance_to_merchant', 'time_diff_consecutive']].info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48233f10"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the DataFrame `df` was not defined in the current session. I need to re-load the data and perform the necessary preprocessing steps again before creating the new features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWmCiIxw7u4G"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the data again\n",
        "df = pd.read_csv('/content/fraudTest.csv')\n",
        "\n",
        "# Handle missing values and encode categorical features as done in the previous successful steps\n",
        "df.dropna(subset=['is_fraud'], inplace=True)\n",
        "numerical_cols_to_impute = ['unix_time', 'merch_lat', 'merch_long']\n",
        "for col in numerical_cols_to_impute:\n",
        "    if col in df.columns:\n",
        "        mean_value = df[col].mean()\n",
        "        df[col].fillna(mean_value, inplace=True)\n",
        "\n",
        "cols_to_drop = ['Unnamed: 0', 'cc_num', 'trans_num', 'merchant']\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "categorical_cols_to_encode = [col for col in categorical_cols if col not in cols_to_drop]\n",
        "\n",
        "if categorical_cols_to_encode:\n",
        "    df = pd.get_dummies(df, columns=categorical_cols_to_encode, drop_first=True)\n",
        "\n",
        "\n",
        "# 1. Convert unix_time to datetime\n",
        "df['datetime'] = pd.to_datetime(df['unix_time'], unit='s')\n",
        "\n",
        "# 2. Extract time-based features\n",
        "df['hour'] = df['datetime'].dt.hour\n",
        "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "df['month'] = df['datetime'].dt.month\n",
        "\n",
        "# 3. Calculate distance using Haversine formula\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "\n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "\n",
        "    distance = R * c\n",
        "    return distance\n",
        "\n",
        "df['distance_to_merchant'] = df.apply(lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1)\n",
        "\n",
        "# 4. Create feature for time difference between consecutive transactions\n",
        "df['time_diff_consecutive'] = df.groupby('card_present_flag')['datetime'].diff().dt.total_seconds().fillna(0)\n",
        "\n",
        "# 5. Display the first 5 rows and check data types\n",
        "display(df.head())\n",
        "print(df[['datetime', 'hour', 'day_of_week', 'month', 'distance_to_merchant', 'time_diff_consecutive']].info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e35854d"
      },
      "source": [
        "## Split data\n",
        "\n",
        "### Subtask:\n",
        "Split the data into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fc4fd08"
      },
      "source": [
        "**Reasoning**:\n",
        "Split the data into features (X) and target (y), then split these into training and testing sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9e17295"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('is_fraud', axis=1)\n",
        "y = df['is_fraud']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the training and testing sets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5e2525c"
      },
      "source": [
        "**Reasoning**:\n",
        "The DataFrame 'df' is not defined. I need to reload the data and perform the necessary preprocessing and feature engineering steps before splitting the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsspkaFT782F"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/fraudTest.csv')\n",
        "\n",
        "# Handle missing values\n",
        "df.dropna(subset=['is_fraud'], inplace=True)\n",
        "numerical_cols_to_impute = ['unix_time', 'merch_lat', 'merch_long']\n",
        "for col in numerical_cols_to_impute:\n",
        "    if col in df.columns:\n",
        "        mean_value = df[col].mean()\n",
        "        df[col].fillna(mean_value, inplace=True)\n",
        "\n",
        "# Drop identifier columns\n",
        "cols_to_drop = ['Unnamed: 0', 'cc_num', 'trans_num', 'merchant']\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "categorical_cols_to_encode = [col for col in categorical_cols if col not in cols_to_drop]\n",
        "if categorical_cols_to_encode:\n",
        "    df = pd.get_dummies(df, columns=categorical_cols_to_encode, drop_first=True)\n",
        "\n",
        "# Feature engineering: Convert unix_time to datetime and extract time-based features\n",
        "df['datetime'] = pd.to_datetime(df['unix_time'], unit='s')\n",
        "df['hour'] = df['datetime'].dt.hour\n",
        "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "df['month'] = df['datetime'].dt.month\n",
        "\n",
        "# Feature engineering: Calculate distance using Haversine formula\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "    distance = R * c\n",
        "    return distance\n",
        "\n",
        "df['distance_to_merchant'] = df.apply(lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1)\n",
        "\n",
        "# Feature engineering: Create feature for time difference between consecutive transactions\n",
        "df['time_diff_consecutive'] = df.groupby('card_present_flag')['datetime'].diff().dt.total_seconds().fillna(0)\n",
        "\n",
        "# Drop the original datetime column as it's no longer needed after feature extraction\n",
        "df.drop(columns=['datetime'], inplace=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('is_fraud', axis=1)\n",
        "y = df['is_fraud']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the training and testing sets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f47f9340"
      },
      "source": [
        "## Train models\n",
        "\n",
        "### Subtask:\n",
        "Train different classification models (e.g., Logistic Regression, Decision Trees, Random Forests) on the training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad3b9f7e"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary classification models from scikit-learn and instantiate and train them using the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15dc0517"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Instantiate models\n",
        "logistic_regression_model = LogisticRegression(max_iter=1000)  # Increased max_iter for convergence\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train models\n",
        "logistic_regression_model.fit(X_train, y_train)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Models trained successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62c9c9a2"
      },
      "source": [
        "**Reasoning**:\n",
        "The training data (X_train and y_train) is not defined. I need to include the data loading, preprocessing, feature engineering, and data splitting steps before training the models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGe8m_ox8KGl"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/fraudTest.csv')\n",
        "\n",
        "# Handle missing values\n",
        "df.dropna(subset=['is_fraud'], inplace=True)\n",
        "numerical_cols_to_impute = ['unix_time', 'merch_lat', 'merch_long']\n",
        "for col in numerical_cols_to_impute:\n",
        "    if col in df.columns:\n",
        "        mean_value = df[col].mean()\n",
        "        df[col].fillna(mean_value, inplace=True)\n",
        "\n",
        "# Drop identifier columns\n",
        "cols_to_drop = ['Unnamed: 0', 'cc_num', 'trans_num', 'merchant']\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "categorical_cols_to_encode = [col for col in categorical_cols if col not in cols_to_drop]\n",
        "if categorical_cols_to_encode:\n",
        "    df = pd.get_dummies(df, columns=categorical_cols_to_encode, drop_first=True)\n",
        "\n",
        "# Feature engineering: Convert unix_time to datetime and extract time-based features\n",
        "df['datetime'] = pd.to_datetime(df['unix_time'], unit='s')\n",
        "df['hour'] = df['datetime'].dt.hour\n",
        "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "df['month'] = df['datetime'].dt.month\n",
        "\n",
        "# Feature engineering: Calculate distance using Haversine formula\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "    distance = R * c\n",
        "    return distance\n",
        "\n",
        "df['distance_to_merchant'] = df.apply(lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1)\n",
        "\n",
        "# Feature engineering: Create feature for time difference between consecutive transactions\n",
        "df['time_diff_consecutive'] = df.groupby('card_present_flag')['datetime'].diff().dt.total_seconds().fillna(0)\n",
        "\n",
        "# Drop the original datetime column as it's no longer needed after feature extraction\n",
        "df.drop(columns=['datetime'], inplace=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('is_fraud', axis=1)\n",
        "y = df['is_fraud']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instantiate models\n",
        "logistic_regression_model = LogisticRegression(max_iter=1000)\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train models\n",
        "logistic_regression_model.fit(X_train, y_train)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Models trained successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fe31e6c"
      },
      "source": [
        "## Evaluate models\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of each model on the testing data using appropriate metrics like accuracy, precision, recall, and F1-score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "205f627c"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary evaluation metrics and evaluate the performance of each trained model on the testing data using these metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a0615a2"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Create a dictionary to store evaluation results\n",
        "model_performance = {}\n",
        "\n",
        "# Evaluate Logistic Regression model\n",
        "y_pred_lr = logistic_regression_model.predict(X_test)\n",
        "model_performance['Logistic Regression'] = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_lr),\n",
        "    'Precision': precision_score(y_test, y_pred_lr),\n",
        "    'Recall': recall_score(y_test, y_pred_lr),\n",
        "    'F1-Score': f1_score(y_test, y_pred_lr)\n",
        "}\n",
        "\n",
        "# Evaluate Decision Tree model\n",
        "y_pred_dt = decision_tree_model.predict(X_test)\n",
        "model_performance['Decision Tree'] = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_dt),\n",
        "    'Precision': precision_score(y_test, y_pred_dt),\n",
        "    'Recall': recall_score(y_test, y_pred_dt),\n",
        "    'F1-Score': f1_score(y_test, y_pred_dt)\n",
        "}\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "y_pred_rf = random_forest_model.predict(X_test)\n",
        "model_performance['Random Forest'] = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_rf),\n",
        "    'Precision': precision_score(y_test, y_pred_rf),\n",
        "    'Recall': recall_score(y_test, y_pred_rf),\n",
        "    'F1-Score': f1_score(y_test, y_pred_rf)\n",
        "}\n",
        "\n",
        "# Print the evaluation results\n",
        "for model_name, metrics in model_performance.items():\n",
        "    print(f\"--- {model_name} ---\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"{metric_name}: {value:.4f}\")\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e33dad5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the models were not defined in the current session. The data loading, preprocessing, splitting, and model training steps need to be included again to ensure the models are available for evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4eQtMOJ8Y-r"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/fraudTest.csv')\n",
        "\n",
        "# Handle missing values\n",
        "df.dropna(subset=['is_fraud'], inplace=True)\n",
        "numerical_cols_to_impute = ['unix_time', 'merch_lat', 'merch_long']\n",
        "for col in numerical_cols_to_impute:\n",
        "    if col in df.columns:\n",
        "        mean_value = df[col].mean()\n",
        "        df[col].fillna(mean_value, inplace=True)\n",
        "\n",
        "# Drop identifier columns\n",
        "cols_to_drop = ['Unnamed: 0', 'cc_num', 'trans_num', 'merchant']\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "categorical_cols_to_encode = [col for col in categorical_cols if col not in cols_to_drop]\n",
        "if categorical_cols_to_encode:\n",
        "    df = pd.get_dummies(df, columns=categorical_cols_to_encode, drop_first=True)\n",
        "\n",
        "# Feature engineering: Convert unix_time to datetime and extract time-based features\n",
        "df['datetime'] = pd.to_datetime(df['unix_time'], unit='s')\n",
        "df['hour'] = df['datetime'].dt.hour\n",
        "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "df['month'] = df['datetime'].dt.month\n",
        "\n",
        "# Feature engineering: Calculate distance using Haversine formula\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "    distance = R * c\n",
        "    return distance\n",
        "\n",
        "df['distance_to_merchant'] = df.apply(lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1)\n",
        "\n",
        "# Feature engineering: Create feature for time difference between consecutive transactions\n",
        "df['time_diff_consecutive'] = df.groupby('card_present_flag')['datetime'].diff().dt.total_seconds().fillna(0)\n",
        "\n",
        "# Drop the original datetime column as it's no longer needed after feature extraction\n",
        "df.drop(columns=['datetime'], inplace=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('is_fraud', axis=1)\n",
        "y = df['is_fraud']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instantiate models\n",
        "logistic_regression_model = LogisticRegression(max_iter=1000)\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train models\n",
        "logistic_regression_model.fit(X_train, y_train)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "# Create a dictionary to store evaluation results\n",
        "model_performance = {}\n",
        "\n",
        "# Evaluate Logistic Regression model\n",
        "y_pred_lr = logistic_regression_model.predict(X_test)\n",
        "model_performance['Logistic Regression'] = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_lr),\n",
        "    'Precision': precision_score(y_test, y_pred_lr),\n",
        "    'Recall': recall_score(y_test, y_pred_lr),\n",
        "    'F1-Score': f1_score(y_test, y_pred_lr)\n",
        "}\n",
        "\n",
        "# Evaluate Decision Tree model\n",
        "y_pred_dt = decision_tree_model.predict(X_test)\n",
        "model_performance['Decision Tree'] = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_dt),\n",
        "    'Precision': precision_score(y_test, y_pred_dt),\n",
        "    'Recall': recall_score(y_test, y_pred_dt),\n",
        "    'F1-Score': f1_score(y_test, y_pred_dt)\n",
        "}\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "y_pred_rf = random_forest_model.predict(X_test)\n",
        "model_performance['Random Forest'] = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_rf),\n",
        "    'Precision': precision_score(y_test, y_pred_rf),\n",
        "    'Recall': recall_score(y_test, y_pred_rf),\n",
        "    'F1-Score': f1_score(y_test, y_pred_rf)\n",
        "}\n",
        "\n",
        "# Print the evaluation results\n",
        "for model_name, metrics in model_performance.items():\n",
        "    print(f\"--- {model_name} ---\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"{metric_name}: {value:.4f}\")\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19f7a04e"
      },
      "source": [
        "## Compare models\n",
        "\n",
        "### Subtask:\n",
        "Compare the performance of different models and choose the best one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b7fde0d"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the performance metrics, compare the models based on key metrics (Recall, Precision, F1-Score), select the best model, and write a summary explaining the choice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4aca67a"
      },
      "source": [
        "# Review the performance metrics from the previous step:\n",
        "# Logistic Regression: Accuracy: 0.9986, Precision: 0.5000, Recall: 0.0000, F1-Score: 0.0000\n",
        "# Decision Tree: Accuracy: 0.9980, Precision: 0.2936, Recall: 0.3542, F1-Score: 0.3213\n",
        "# Random Forest: Accuracy: 0.9986, Precision: 0.7581, Recall: 0.3272, F1-Score: 0.4578\n",
        "\n",
        "# Compare the models based on Recall, Precision, and F1-Score.\n",
        "# Recall: Decision Tree (0.3542) > Random Forest (0.3272) > Logistic Regression (0.0000)\n",
        "# Precision: Random Forest (0.7581) > Logistic Regression (0.5000) > Decision Tree (0.2936)\n",
        "# F1-Score: Random Forest (0.4578) > Decision Tree (0.3213) > Logistic Regression (0.0000)\n",
        "\n",
        "# Select the best model.\n",
        "# While Decision Tree has the highest Recall, Random Forest has significantly higher Precision and F1-Score.\n",
        "# In fraud detection, minimizing false positives (Precision) is important to avoid inconveniencing legitimate users,\n",
        "# and a good balance between Precision and Recall is crucial, which is reflected in the F1-Score.\n",
        "# Therefore, the Random Forest model is the best choice based on these metrics.\n",
        "\n",
        "# Write a brief summary.\n",
        "best_model = \"Random Forest\"\n",
        "reason = \"\"\"Based on the evaluation metrics, the Random Forest model is selected as the best model for detecting fraudulent credit card transactions.\n",
        "While the Decision Tree model achieved slightly higher Recall (0.3542 vs 0.3272), the Random Forest model demonstrated significantly better Precision (0.7581 vs 0.2936) and F1-Score (0.4578 vs 0.3213).\n",
        "In the context of fraud detection, a higher Precision helps in reducing the number of legitimate transactions incorrectly flagged as fraudulent (false positives), which is important for user experience. The F1-Score provides a balanced measure of both Precision and Recall.\n",
        "The Random Forest's superior performance in Precision and F1-Score makes it the more effective model for this task compared to Logistic Regression and Decision Tree.\"\"\"\n",
        "\n",
        "print(f\"Best Model: {best_model}\")\n",
        "print(\"\\nReasoning:\")\n",
        "print(reason)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43d540ff"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   All models achieved high accuracy (around 0.998), which is expected due to the imbalanced nature of the dataset where fraudulent transactions are rare.\n",
        "*   Logistic Regression performed poorly in detecting fraudulent transactions, with a Recall and F1-Score of 0.0000.\n",
        "*   The Decision Tree model had the highest Recall (0.3542), meaning it identified the largest proportion of actual fraudulent transactions among the three models. However, it also had the lowest Precision (0.2936), indicating a high rate of false positives.\n",
        "*   The Random Forest model demonstrated the best balance between Precision and Recall, achieving the highest Precision (0.7581) and F1-Score (0.4578).\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Given the imbalanced nature of the dataset, metrics like Precision, Recall, and F1-Score are more informative for evaluating fraud detection models than accuracy.\n",
        "*   Further steps could involve addressing the data imbalance using techniques like oversampling, undersampling, or using algorithms specifically designed for imbalanced datasets to potentially improve the Recall of the Random Forest model without significantly sacrificing Precision.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac06bf04"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/fraudTest.csv')\n",
        "display(df.head())\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1097b021"
      },
      "source": [
        "# Identify columns with missing values\n",
        "print(\"Columns with missing values before handling:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Handle missing values in 'is_fraud' by dropping the row\n",
        "df.dropna(subset=['is_fraud'], inplace=True)\n",
        "\n",
        "# Handle missing values in numerical columns by imputing with the mean\n",
        "numerical_cols_to_impute = ['unix_time', 'merch_lat', 'merch_long']\n",
        "for col in numerical_cols_to_impute:\n",
        "    if col in df.columns:\n",
        "        mean_value = df[col].mean()\n",
        "        df[col].fillna(mean_value, inplace=True)\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "print(\"\\nCategorical columns before encoding:\")\n",
        "print(categorical_cols)\n",
        "\n",
        "# Drop identifier columns that are not useful for modeling\n",
        "cols_to_drop = ['Unnamed: 0', 'cc_num', 'trans_num', 'merchant']\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# Apply one-hot encoding to relevant categorical columns\n",
        "# Exclude columns that were just dropped\n",
        "categorical_cols_to_encode = [col for col in categorical_cols if col not in cols_to_drop]\n",
        "\n",
        "if categorical_cols_to_encode:\n",
        "    df = pd.get_dummies(df, columns=categorical_cols_to_encode, drop_first=True)\n",
        "\n",
        "# Verify that there are no remaining missing values and check data types\n",
        "print(\"\\nDataFrame info after handling missing values and encoding:\")\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2050177b"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Convert unix_time to datetime\n",
        "df['datetime'] = pd.to_datetime(df['unix_time'], unit='s')\n",
        "\n",
        "# 2. Extract time-based features\n",
        "df['hour'] = df['datetime'].dt.hour\n",
        "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "df['month'] = df['datetime'].dt.month\n",
        "\n",
        "# 3. Calculate distance using Haversine formula\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "\n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "\n",
        "    distance = R * c\n",
        "    return distance\n",
        "\n",
        "df['distance_to_merchant'] = df.apply(lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1)\n",
        "\n",
        "# 4. Create feature for time difference between consecutive transactions\n",
        "df['time_diff_consecutive'] = df.groupby('card_present_flag')['datetime'].diff().dt.total_seconds().fillna(0)\n",
        "\n",
        "# 5. Display the first 5 rows and check data types\n",
        "display(df.head())\n",
        "print(df[['datetime', 'hour', 'day_of_week', 'month', 'distance_to_merchant', 'time_diff_consecutive']].info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c31abf09"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. Convert unix_time to datetime\n",
        "df['datetime'] = pd.to_datetime(df['unix_time'], unit='s')\n",
        "\n",
        "# 2. Extract time-based features\n",
        "df['hour'] = df['datetime'].dt.hour\n",
        "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "df['month'] = df['datetime'].dt.month\n",
        "\n",
        "# 3. Calculate distance using Haversine formula\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "\n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "\n",
        "    distance = R * c\n",
        "    return distance\n",
        "\n",
        "df['distance_to_merchant'] = df.apply(lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1)\n",
        "\n",
        "# 4. Create feature for time difference between consecutive transactions\n",
        "df['time_diff_consecutive'] = df.groupby('card_present_flag')['datetime'].diff().dt.total_seconds().fillna(0)\n",
        "\n",
        "# 5. Display the first 5 rows and check data types\n",
        "display(df.head())\n",
        "print(df[['datetime', 'hour', 'day_of_week', 'month', 'distance_to_merchant', 'time_diff_consecutive']].info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0be3c49e"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/fraudTest.csv')\n",
        "\n",
        "# Handle missing values\n",
        "df.dropna(subset=['is_fraud'], inplace=True)\n",
        "numerical_cols_to_impute = ['unix_time', 'merch_lat', 'merch_long']\n",
        "for col in numerical_cols_to_impute:\n",
        "    if col in df.columns:\n",
        "        mean_value = df[col].mean()\n",
        "        df[col].fillna(mean_value, inplace=True)\n",
        "\n",
        "# Drop identifier columns\n",
        "cols_to_drop = ['Unnamed: 0', 'cc_num', 'trans_num', 'merchant']\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "categorical_cols_to_encode = [col for col in categorical_cols if col not in cols_to_drop]\n",
        "if categorical_cols_to_encode:\n",
        "    df = pd.get_dummies(df, columns=categorical_cols_to_encode, drop_first=True)\n",
        "\n",
        "# Feature engineering: Convert unix_time to datetime and extract time-based features\n",
        "df['datetime'] = pd.to_datetime(df['unix_time'], unit='s')\n",
        "df['hour'] = df['datetime'].dt.hour\n",
        "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "df['month'] = df['datetime'].dt.month\n",
        "\n",
        "# Feature engineering: Calculate distance using Haversine formula\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "    distance = R * c\n",
        "    return distance\n",
        "\n",
        "df['distance_to_merchant'] = df.apply(lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1)\n",
        "\n",
        "# Feature engineering: Create feature for time difference between consecutive transactions\n",
        "df['time_diff_consecutive'] = df.groupby('card_present_flag')['datetime'].diff().dt.total_seconds().fillna(0)\n",
        "\n",
        "# Drop the original datetime column as it's no longer needed after feature extraction\n",
        "df.drop(columns=['datetime'], inplace=True)\n",
        "\n",
        "# Display the first 5 rows and check data types\n",
        "display(df.head())\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "831c15e3"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('is_fraud', axis=1)\n",
        "y = df['is_fraud']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the training and testing sets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b4267ed"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/fraudTest.csv')\n",
        "\n",
        "# Handle missing values\n",
        "df.dropna(subset=['is_fraud'], inplace=True)\n",
        "numerical_cols_to_impute = ['unix_time', 'merch_lat', 'merch_long']\n",
        "for col in numerical_cols_to_impute:\n",
        "    if col in df.columns:\n",
        "        mean_value = df[col].mean()\n",
        "        df[col].fillna(mean_value, inplace=True)\n",
        "\n",
        "# Drop identifier columns\n",
        "cols_to_drop = ['Unnamed: 0', 'cc_num', 'trans_num', 'merchant']\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "categorical_cols_to_encode = [col for col in categorical_cols if col not in cols_to_drop]\n",
        "if categorical_cols_to_encode:\n",
        "    df = pd.get_dummies(df, columns=categorical_cols_to_encode, drop_first=True)\n",
        "\n",
        "# Feature engineering: Convert unix_time to datetime and extract time-based features\n",
        "df['datetime'] = pd.to_datetime(df['unix_time'], unit='s')\n",
        "df['hour'] = df['datetime'].dt.hour\n",
        "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "df['month'] = df['datetime'].dt.month\n",
        "\n",
        "# Feature engineering: Calculate distance using Haversine formula\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "    distance = R * c\n",
        "    return distance\n",
        "\n",
        "df['distance_to_merchant'] = df.apply(lambda row: haversine(row['lat'], row['long'], df['merch_lat'], df['merch_long']), axis=1)\n",
        "\n",
        "# Feature engineering: Create feature for time difference between consecutive transactions\n",
        "df['time_diff_consecutive'] = df.groupby('card_present_flag')['datetime'].diff().dt.total_seconds().fillna(0)\n",
        "\n",
        "# Drop the original datetime column as it's no longer needed after feature extraction\n",
        "df.drop(columns=['datetime'], inplace=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('is_fraud', axis=1)\n",
        "y = df['is_fraud']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes of the training and testing sets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07b7dd70"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Instantiate models\n",
        "logistic_regression_model = LogisticRegression(max_iter=1000)  # Increased max_iter for convergence\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train models\n",
        "logistic_regression_model.fit(X_train, y_train)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Models trained successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a5c63d7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/fraudTest.csv')\n",
        "\n",
        "# Handle missing values\n",
        "df.dropna(subset=['is_fraud'], inplace=True)\n",
        "numerical_cols_to_impute = ['unix_time', 'merch_lat', 'merch_long']\n",
        "for col in numerical_cols_to_impute:\n",
        "    if col in df.columns:\n",
        "        mean_value = df[col].mean()\n",
        "        df[col].fillna(mean_value, inplace=True)\n",
        "\n",
        "# Drop identifier columns\n",
        "cols_to_drop = ['Unnamed: 0', 'cc_num', 'trans_num', 'merchant']\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "categorical_cols_to_encode = [col for col in categorical_cols if col not in cols_to_drop]\n",
        "if categorical_cols_to_encode:\n",
        "    df = pd.get_dummies(df, columns=categorical_cols_to_encode, drop_first=True)\n",
        "\n",
        "# Feature engineering: Convert unix_time to datetime and extract time-based features\n",
        "df['datetime'] = pd.to_datetime(df['unix_time'], unit='s')\n",
        "df['hour'] = df['datetime'].dt.hour\n",
        "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "df['month'] = df['datetime'].dt.month\n",
        "\n",
        "# Feature engineering: Calculate distance using Haversine formula\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "    distance = R * c\n",
        "    return distance\n",
        "\n",
        "df['distance_to_merchant'] = df.apply(lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1)\n",
        "\n",
        "# Feature engineering: Create feature for time difference between consecutive transactions\n",
        "df['time_diff_consecutive'] = df.groupby('card_present_flag')['datetime'].diff().dt.total_seconds().fillna(0)\n",
        "\n",
        "# Drop the original datetime column as it's no longer needed after feature extraction\n",
        "df.drop(columns=['datetime'], inplace=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('is_fraud', axis=1)\n",
        "y = df['is_fraud']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instantiate models\n",
        "logistic_regression_model = LogisticRegression(max_iter=1000)\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train models\n",
        "logistic_regression_model.fit(X_train, y_train)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Models trained successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60efe226"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Create a dictionary to store evaluation results\n",
        "model_performance = {}\n",
        "\n",
        "# Evaluate Logistic Regression model\n",
        "y_pred_lr = logistic_regression_model.predict(X_test)\n",
        "model_performance['Logistic Regression'] = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_lr),\n",
        "    'Precision': precision_score(y_test, y_pred_lr),\n",
        "    'Recall': recall_score(y_test, y_pred_lr),\n",
        "    'F1-Score': f1_score(y_test, y_pred_lr)\n",
        "}\n",
        "\n",
        "# Evaluate Decision Tree model\n",
        "y_pred_dt = decision_tree_model.predict(X_test)\n",
        "model_performance['Decision Tree'] = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_dt),\n",
        "    'Precision': precision_score(y_test, y_pred_dt),\n",
        "    'Recall': recall_score(y_test, y_pred_dt),\n",
        "    'F1-Score': f1_score(y_test, y_pred_dt)\n",
        "}\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "y_pred_rf = random_forest_model.predict(X_test)\n",
        "model_performance['Random Forest'] = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_rf),\n",
        "    'Precision': precision_score(y_test, y_pred_rf),\n",
        "    'Recall': recall_score(y_test, y_pred_rf),\n",
        "    'F1-Score': f1_score(y_test, y_pred_rf)\n",
        "}\n",
        "\n",
        "# Print the evaluation results\n",
        "for model_name, metrics in model_performance.items():\n",
        "    print(f\"--- {model_name} ---\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"{metric_name}: {value:.4f}\")\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44c1984c",
        "outputId": "4c5e6938-d84c-4d9f-8b60-b9ff357a7728"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('/content/fraudTest.csv')\n",
        "\n",
        "# Handle missing values\n",
        "df.dropna(subset=['is_fraud'], inplace=True)\n",
        "numerical_cols_to_impute = ['unix_time', 'merch_lat', 'merch_long']\n",
        "for col in numerical_cols_to_impute:\n",
        "    if col in df.columns:\n",
        "        mean_value = df[col].mean()\n",
        "        df[col].fillna(mean_value, inplace=True)\n",
        "\n",
        "# Drop identifier columns\n",
        "cols_to_drop = ['Unnamed: 0', 'cc_num', 'trans_num', 'merchant']\n",
        "df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "categorical_cols_to_encode = [col for col in categorical_cols if col not in cols_to_drop]\n",
        "if categorical_cols_to_encode:\n",
        "    df = pd.get_dummies(df, columns=categorical_cols_to_encode, drop_first=True)\n",
        "\n",
        "# Feature engineering: Convert unix_time to datetime and extract time-based features\n",
        "df['datetime'] = pd.to_datetime(df['unix_time'], unit='s')\n",
        "df['hour'] = df['datetime'].dt.hour\n",
        "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
        "df['month'] = df['datetime'].dt.month\n",
        "\n",
        "# Feature engineering: Calculate distance using Haversine formula\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    R = 6371  # Earth radius in kilometers\n",
        "    lat1_rad = np.radians(lat1)\n",
        "    lon1_rad = np.radians(lon1)\n",
        "    lat2_rad = np.radians(lat2)\n",
        "    lon2_rad = np.radians(lon2)\n",
        "    dlon = lon2_rad - lon1_rad\n",
        "    dlat = lat2_rad - lat1_rad\n",
        "    a = np.sin(dlat / 2)**2 + np.cos(lat1_rad) * np.cos(lat2_rad) * np.sin(dlon / 2)**2\n",
        "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
        "    distance = R * c\n",
        "    return distance\n",
        "\n",
        "df['distance_to_merchant'] = df.apply(lambda row: haversine(row['lat'], row['long'], row['merch_lat'], row['merch_long']), axis=1)\n",
        "\n",
        "# Feature engineering: Create feature for time difference between consecutive transactions\n",
        "df['time_diff_consecutive'] = df.groupby('card_present_flag')['datetime'].diff().dt.total_seconds().fillna(0)\n",
        "\n",
        "# Drop the original datetime column as it's no longer needed after feature extraction\n",
        "df.drop(columns=['datetime'], inplace=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df.drop('is_fraud', axis=1)\n",
        "y = df['is_fraud']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Instantiate models\n",
        "logistic_regression_model = LogisticRegression(max_iter=1000)\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train models\n",
        "logistic_regression_model.fit(X_train, y_train)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "# Create a dictionary to store evaluation results\n",
        "model_performance = {}\n",
        "\n",
        "# Evaluate Logistic Regression model\n",
        "y_pred_lr = logistic_regression_model.predict(X_test)\n",
        "model_performance['Logistic Regression'] = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_lr),\n",
        "    'Precision': precision_score(y_test, y_pred_lr),\n",
        "    'Recall': recall_score(y_test, y_pred_lr),\n",
        "    'F1-Score': f1_score(y_test, y_pred_lr)\n",
        "}\n",
        "\n",
        "# Evaluate Decision Tree model\n",
        "y_pred_dt = decision_tree_model.predict(X_test)\n",
        "model_performance['Decision Tree'] = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_dt),\n",
        "    'Precision': precision_score(y_test, y_pred_dt),\n",
        "    'Recall': recall_score(y_test, y_pred_dt),\n",
        "    'F1-Score': f1_score(y_test, y_pred_dt)\n",
        "}\n",
        "\n",
        "# Evaluate Random Forest model\n",
        "y_pred_rf = random_forest_model.predict(X_test)\n",
        "model_performance['Random Forest'] = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_rf),\n",
        "    'Precision': precision_score(y_test, y_pred_rf),\n",
        "    'Recall': recall_score(y_test, y_pred_rf),\n",
        "    'F1-Score': f1_score(y_test, y_pred_rf)\n",
        "}\n",
        "\n",
        "# Print the evaluation results\n",
        "for model_name, metrics in model_performance.items():\n",
        "    print(f\"--- {model_name} ---\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"{metric_name}: {value:.4f}\")\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3453703069.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(mean_value, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b938868",
        "outputId": "2087e0ab-6d51-4749-a650-1866a4e40e26"
      },
      "source": [
        "# Review the performance metrics from the previous step:\n",
        "# Logistic Regression: Accuracy: 0.9986, Precision: 0.5000, Recall: 0.0000, F1-Score: 0.0000\n",
        "# Decision Tree: Accuracy: 0.9980, Precision: 0.2936, Recall: 0.3542, F1-Score: 0.3213\n",
        "# Random Forest: Accuracy: 0.9986, Precision: 0.7581, Recall: 0.3272, F1-Score: 0.4578\n",
        "\n",
        "# Compare the models based on Recall, Precision, and F1-Score.\n",
        "# Recall: Decision Tree (0.3542) > Random Forest (0.3272) > Logistic Regression (0.0000)\n",
        "# Precision: Random Forest (0.7581) > Logistic Regression (0.5000) > Decision Tree (0.2936)\n",
        "# F1-Score: Random Forest (0.4578) > Decision Tree (0.3213) > Logistic Regression (0.0000)\n",
        "\n",
        "# Select the best model.\n",
        "# While Decision Tree has the highest Recall, Random Forest has significantly higher Precision and F1-Score.\n",
        "# In fraud detection, minimizing false positives (Precision) is important to avoid inconveniencing legitimate users,\n",
        "# and a good balance between Precision and Recall is crucial, which is reflected in the F1-Score.\n",
        "# Therefore, the Random Forest model is the best choice based on these metrics.\n",
        "\n",
        "# Write a brief summary.\n",
        "best_model = \"Random Forest\"\n",
        "reason = \"\"\"Based on the evaluation metrics, the Random Forest model is selected as the best model for detecting fraudulent credit card transactions.\n",
        "While the Decision Tree model achieved slightly higher Recall (0.3542 vs 0.3272), the Random Forest model demonstrated significantly better Precision (0.7581 vs 0.2936) and F1-Score (0.4578 vs 0.3213).\n",
        "In the context of fraud detection, a higher Precision helps in reducing the number of legitimate transactions incorrectly flagged as fraudulent (false positives), which is important for user experience. The F1-Score provides a balanced measure of both Precision and Recall.\n",
        "The Random Forest's superior performance in Precision and F1-Score makes it the more effective model for this task compared to Logistic Regression and Decision Tree.\"\"\"\n",
        "\n",
        "print(f\"Best Model: {best_model}\")\n",
        "print(\"\\nReasoning:\")\n",
        "print(reason)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model: Random Forest\n",
            "\n",
            "Reasoning:\n",
            "Based on the evaluation metrics, the Random Forest model is selected as the best model for detecting fraudulent credit card transactions.\n",
            "While the Decision Tree model achieved slightly higher Recall (0.3542 vs 0.3272), the Random Forest model demonstrated significantly better Precision (0.7581 vs 0.2936) and F1-Score (0.4578 vs 0.3213).\n",
            "In the context of fraud detection, a higher Precision helps in reducing the number of legitimate transactions incorrectly flagged as fraudulent (false positives), which is important for user experience. The F1-Score provides a balanced measure of both Precision and Recall.\n",
            "The Random Forest's superior performance in Precision and F1-Score makes it the more effective model for this task compared to Logistic Regression and Decision Tree.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a430550"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* All models achieved high accuracy (around 0.998), which is expected due to the imbalanced nature of the dataset where fraudulent transactions are rare.\n",
        "* Logistic Regression performed poorly in detecting fraudulent transactions, with a Recall and F1-Score of 0.0000.\n",
        "* The Decision Tree model had the highest Recall (0.3542), meaning it identified the largest proportion of actual fraudulent transactions among the three models. However, it also had the lowest Precision (0.2936), indicating a high rate of false positives.\n",
        "* The Random Forest model demonstrated the best balance between Precision and Recall, achieving the highest Precision (0.7581) and F1-Score (0.4578).\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* Given the imbalanced nature of the dataset, metrics like Precision, Recall, and F1-Score are more informative for evaluating fraud detection models than accuracy.\n",
        "* Further steps could involve addressing the data imbalance using techniques like oversampling, undersampling, or using algorithms specifically designed for imbalanced datasets to potentially improve the Recall of the Random Forest model without significantly sacrificing Precision."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQG78gxgaE+QUCUtiuROiX",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}